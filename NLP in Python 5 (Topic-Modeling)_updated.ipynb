{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaah</th>\n",
       "      <th>aarp</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>ability</th>\n",
       "      <th>abki</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>...</th>\n",
       "      <th>zhoosh</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>álvarez</th>\n",
       "      <th>ándale</th>\n",
       "      <th>ñañaras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 6583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaah  aarp  abc  abdomen  abducted  abhorring  ability  abki  able  \\\n",
       "bill         0     1    0        0         0          0        1     0     1   \n",
       "brian        0     0    0        0         0          0        0     0     1   \n",
       "dave         1     0    0        0         0          0        0     0     0   \n",
       "fortune      0     0    0        0         0          0        0     0     2   \n",
       "gabriel      1     0    1        0         0          0        0     0     0   \n",
       "jim          0     0    0        1         0          0        0     0     0   \n",
       "kathleen     0     0    0        0         1          1        0     0     1   \n",
       "oswalt       0     0    0        0         0          0        0     0     2   \n",
       "vir          0     0    0        0         1          0        1     1     0   \n",
       "\n",
       "          aboard  ...  zhoosh  zip  zipper  zoned  zones  zoom  zoomed  \\\n",
       "bill           0  ...       0    0       0      1      0     2       0   \n",
       "brian          0  ...       0    0       1      0      0     0       0   \n",
       "dave           0  ...       0    0       0      0      0     0       0   \n",
       "fortune        0  ...       0    0       0      0      0     1       1   \n",
       "gabriel        0  ...       0    0       0      0      1     4       0   \n",
       "jim            0  ...       0    0       0      0      0     0       0   \n",
       "kathleen       0  ...       0    1       0      0      0     0       0   \n",
       "oswalt         1  ...       1    0       0      0      0     2       0   \n",
       "vir            0  ...       0    0       0      0      0     0       0   \n",
       "\n",
       "          álvarez  ándale  ñañaras  \n",
       "bill            0       0        0  \n",
       "brian           0       0        0  \n",
       "dave            0       0        0  \n",
       "fortune         0       0        0  \n",
       "gabriel         3       1        1  \n",
       "jim             0       0        0  \n",
       "kathleen        0       0        0  \n",
       "oswalt          0       0        0  \n",
       "vir             0       0        0  \n",
       "\n",
       "[9 rows x 6583 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>brian</th>\n",
       "      <th>dave</th>\n",
       "      <th>fortune</th>\n",
       "      <th>gabriel</th>\n",
       "      <th>jim</th>\n",
       "      <th>kathleen</th>\n",
       "      <th>oswalt</th>\n",
       "      <th>vir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarp</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdomen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abducted</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bill  brian  dave  fortune  gabriel  jim  kathleen  oswalt  vir\n",
       "aaah         0      0     1        0        1    0         0       0    0\n",
       "aarp         1      0     0        0        0    0         0       0    0\n",
       "abc          0      0     0        0        1    0         0       0    0\n",
       "abdomen      0      0     0        0        0    1         0       0    0\n",
       "abducted     0      0     0        0        0    0         1       0    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"audience\" + 0.013*\"laughs\" + 0.005*\"theyre\" + 0.005*\"said\" + 0.004*\"horse\" + 0.004*\"look\" + 0.004*\"theres\" + 0.003*\"want\" + 0.003*\"goes\" + 0.003*\"love\"'),\n",
       " (1,\n",
       "  '0.008*\"said\" + 0.007*\"shit\" + 0.005*\"going\" + 0.005*\"time\" + 0.005*\"fuckin\" + 0.005*\"fuck\" + 0.005*\"didnt\" + 0.004*\"okay\" + 0.004*\"hes\" + 0.004*\"say\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"said\" + 0.005*\"going\" + 0.004*\"uh\" + 0.004*\"theres\" + 0.004*\"theyre\" + 0.004*\"really\" + 0.004*\"say\" + 0.004*\"goes\" + 0.004*\"thought\" + 0.004*\"day\"'),\n",
       " (1,\n",
       "  '0.012*\"audience\" + 0.010*\"laughs\" + 0.007*\"said\" + 0.005*\"going\" + 0.005*\"look\" + 0.005*\"theyre\" + 0.004*\"okay\" + 0.004*\"hes\" + 0.004*\"want\" + 0.004*\"theres\"'),\n",
       " (2,\n",
       "  '0.011*\"fuckin\" + 0.010*\"shit\" + 0.007*\"fuck\" + 0.007*\"said\" + 0.007*\"time\" + 0.006*\"man\" + 0.006*\"didnt\" + 0.004*\"fucking\" + 0.004*\"women\" + 0.004*\"guys\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"said\" + 0.000*\"going\" + 0.000*\"time\" + 0.000*\"say\" + 0.000*\"okay\" + 0.000*\"audience\" + 0.000*\"tell\" + 0.000*\"thing\" + 0.000*\"theyre\" + 0.000*\"theres\"'),\n",
       " (1,\n",
       "  '0.022*\"audience\" + 0.018*\"laughs\" + 0.006*\"horse\" + 0.005*\"theyre\" + 0.005*\"theres\" + 0.005*\"horses\" + 0.004*\"didnt\" + 0.004*\"look\" + 0.004*\"wanna\" + 0.004*\"kids\"'),\n",
       " (2,\n",
       "  '0.009*\"fuckin\" + 0.006*\"said\" + 0.006*\"going\" + 0.005*\"shit\" + 0.005*\"theyre\" + 0.005*\"fuck\" + 0.004*\"little\" + 0.004*\"theres\" + 0.004*\"say\" + 0.004*\"time\"'),\n",
       " (3,\n",
       "  '0.012*\"said\" + 0.008*\"shit\" + 0.006*\"time\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"guys\" + 0.005*\"hes\" + 0.005*\"want\" + 0.004*\"tell\" + 0.004*\"okay\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>ladies and gentlemen bill burr  all right th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>♪    lets give a big warm welcome to mr brian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>please welcome fortune feimster ♪ im a powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>can you please state your name martin moreno ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>audience cheering applauding thank you thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>whoo   ♪ kathleen ♪ ♪ madigan ♪ ♪ kathleen ♪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>hello denver   oh my god hello thank you tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>i lost  of my mind its very freeing you should...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "bill        ladies and gentlemen bill burr  all right th...\n",
       "brian     ♪    lets give a big warm welcome to mr brian ...\n",
       "dave      this is dave he tells dirty jokes for a living...\n",
       "fortune      please welcome fortune feimster ♪ im a powe...\n",
       "gabriel    can you please state your name martin moreno ...\n",
       "jim       audience cheering applauding thank you thank y...\n",
       "kathleen    whoo   ♪ kathleen ♪ ♪ madigan ♪ ♪ kathleen ♪...\n",
       "oswalt      hello denver   oh my god hello thank you tha...\n",
       "vir       i lost  of my mind its very freeing you should..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>ladies gentlemen bill thank hows sit see year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>lets warm welcome regan thank thanks appreciat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>fortune feimster ♪ woman ♪ ♪ i ♪ ♪ way i ♪ ♪ c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno martinnnnn gabriel ig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>audience thank god thank audience fault fault ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>whoo ♪ ♪ ♪ madigan ♪ ♪ ♪ ♪ madigan ♪ ♪ stage s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>hello denver god hey i foot yeah year i foot p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>i mind look way evening francisco yeah name da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "bill      ladies gentlemen bill thank hows sit see year ...\n",
       "brian     lets warm welcome regan thank thanks appreciat...\n",
       "dave      jokes living stare work profound train thought...\n",
       "fortune   fortune feimster ♪ woman ♪ ♪ i ♪ ♪ way i ♪ ♪ c...\n",
       "gabriel   state name martin moreno martinnnnn gabriel ig...\n",
       "jim       audience thank god thank audience fault fault ...\n",
       "kathleen  whoo ♪ ♪ ♪ madigan ♪ ♪ ♪ ♪ madigan ♪ ♪ stage s...\n",
       "oswalt    hello denver god hey i foot yeah year i foot p...\n",
       "vir       i mind look way evening francisco yeah name da..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>aboriginals</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abraham</th>\n",
       "      <th>ac</th>\n",
       "      <th>academy</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zhoosh</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>álvarez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 4067 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaah  abc  abdomen  ability  aboriginals  abortion  abortions  \\\n",
       "bill         0    0        0        1            0         2          0   \n",
       "brian        0    0        0        0            0         0          0   \n",
       "dave         0    0        0        0            0         0          1   \n",
       "fortune      0    0        0        0            0         0          0   \n",
       "gabriel      1    1        0        0            0         0          0   \n",
       "jim          0    0        1        0            1         0          0   \n",
       "kathleen     0    0        0        0            0         0          0   \n",
       "oswalt       0    0        0        0            0         0          0   \n",
       "vir          0    0        0        1            0         0          0   \n",
       "\n",
       "          abraham  ac  academy  ...  youth  youve  yuck  zealand  zhoosh  zip  \\\n",
       "bill            0   0        0  ...      0      0     0        0       0    0   \n",
       "brian           0   0        0  ...      0      0     0        0       0    0   \n",
       "dave            0   0        0  ...      0      1     0        0       0    0   \n",
       "fortune         0   0        0  ...      0      0     0        0       0    0   \n",
       "gabriel         0   1        6  ...      0      2     0        0       0    0   \n",
       "jim             0   0        0  ...      0      2     0        4       0    0   \n",
       "kathleen        1   0        0  ...      0      4     0        0       0    1   \n",
       "oswalt          0   1        0  ...      1      4     0        0       1    0   \n",
       "vir             0   0        0  ...      0      1     1        0       0    0   \n",
       "\n",
       "          zipper  zones  zoom  álvarez  \n",
       "bill           0      0     2        0  \n",
       "brian          1      0     0        0  \n",
       "dave           0      0     0        0  \n",
       "fortune        0      0     1        0  \n",
       "gabriel        0      1     3        3  \n",
       "jim            0      0     0        0  \n",
       "kathleen       0      0     0        0  \n",
       "oswalt         0      0     2        0  \n",
       "vir            0      0     0        0  \n",
       "\n",
       "[9 rows x 4067 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"shit\" + 0.009*\"gon\" + 0.009*\"man\" + 0.007*\"cause\" + 0.007*\"fuck\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"years\" + 0.005*\"car\" + 0.005*\"way\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.019*\"audience\" + 0.008*\"hes\" + 0.007*\"way\" + 0.006*\"gon\" + 0.006*\"cause\" + 0.006*\"guy\" + 0.006*\"thing\" + 0.005*\"horse\" + 0.005*\"day\" + 0.004*\"house\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "for idx, topic in ldan.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.040*\"audience\" + 0.011*\"horse\" + 0.008*\"horses\" + 0.007*\"laughs\" + 0.007*\"kids\" + 0.006*\"guy\" + 0.006*\"way\" + 0.005*\"dog\" + 0.005*\"gon\" + 0.005*\"cause\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.013*\"shit\" + 0.010*\"gon\" + 0.010*\"fuck\" + 0.008*\"thing\" + 0.008*\"man\" + 0.006*\"day\" + 0.006*\"god\" + 0.006*\"women\" + 0.006*\"shes\" + 0.006*\"way\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"cause\" + 0.008*\"gon\" + 0.008*\"hes\" + 0.007*\"car\" + 0.007*\"man\" + 0.007*\"way\" + 0.006*\"thing\" + 0.006*\"shit\" + 0.006*\"lot\" + 0.006*\"years\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()\n",
    "for idx, topic in ldan.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"cause\" + 0.009*\"gon\" + 0.008*\"car\" + 0.008*\"hes\" + 0.007*\"thing\" + 0.007*\"way\" + 0.006*\"day\" + 0.006*\"years\" + 0.005*\"home\" + 0.005*\"things\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.016*\"shit\" + 0.010*\"fuck\" + 0.010*\"thing\" + 0.009*\"man\" + 0.009*\"gon\" + 0.009*\"women\" + 0.006*\"years\" + 0.006*\"guys\" + 0.006*\"day\" + 0.006*\"god\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.014*\"shit\" + 0.011*\"man\" + 0.009*\"fuck\" + 0.008*\"lot\" + 0.008*\"didnt\" + 0.006*\"ahah\" + 0.006*\"room\" + 0.006*\"money\" + 0.005*\"hes\" + 0.005*\"guy\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.031*\"audience\" + 0.009*\"horse\" + 0.008*\"guy\" + 0.008*\"gon\" + 0.007*\"way\" + 0.006*\"horses\" + 0.006*\"day\" + 0.006*\"hes\" + 0.005*\"house\" + 0.005*\"kids\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "for idx, topic in ldan.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>ladies gentlemen bill right thank ya hows sit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>♪ lets big warm welcome brian regan right than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>welcome fortune feimster ♪ powerful woman ♪ ♪ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno martinnnnn gabriel ig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>audience thank god thank audience good i fault...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>whoo ♪ kathleen ♪ ♪ madigan ♪ ♪ ♪ ♪ madigan ♪ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>hello denver god god much tonight hey i foot y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>i mind look way good evening san francisco guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "bill      ladies gentlemen bill right thank ya hows sit ...\n",
       "brian     ♪ lets big warm welcome brian regan right than...\n",
       "dave      dirty jokes living stare most hard work profou...\n",
       "fortune   welcome fortune feimster ♪ powerful woman ♪ ♪ ...\n",
       "gabriel   state name martin moreno martinnnnn gabriel ig...\n",
       "jim       audience thank god thank audience good i fault...\n",
       "kathleen  whoo ♪ kathleen ♪ ♪ madigan ♪ ♪ ♪ ♪ madigan ♪ ...\n",
       "oswalt    hello denver god god much tonight hey i foot y...\n",
       "vir       i mind look way good evening san francisco guy..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaah</th>\n",
       "      <th>aarp</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>abki</th>\n",
       "      <th>able</th>\n",
       "      <th>aboriginals</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>...</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zenlike</th>\n",
       "      <th>zhoosh</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>álvarez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 4857 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaah  aarp  abc  abdomen  ability  abki  able  aboriginals  \\\n",
       "bill         0     1    0        0        1     0     1            0   \n",
       "brian        0     0    0        0        0     0     1            0   \n",
       "dave         0     0    0        0        0     0     0            0   \n",
       "fortune      0     0    0        0        0     0     2            0   \n",
       "gabriel      1     0    1        0        0     0     0            0   \n",
       "jim          0     0    0        1        0     0     0            1   \n",
       "kathleen     0     0    0        0        0     0     1            0   \n",
       "oswalt       0     0    0        0        0     0     2            0   \n",
       "vir          0     0    0        0        1     1     0            0   \n",
       "\n",
       "          abortion  abortions  ...  youve  yuck  zealand  zenlike  zhoosh  \\\n",
       "bill             2          0  ...      1     0        0        1       0   \n",
       "brian            0          0  ...      0     0        0        0       0   \n",
       "dave             0          1  ...      2     0        0        0       0   \n",
       "fortune          0          0  ...      0     0        0        0       0   \n",
       "gabriel          0          0  ...      2     0        0        0       0   \n",
       "jim              0          0  ...      3     0        4        0       0   \n",
       "kathleen         0          0  ...      4     0        0        0       0   \n",
       "oswalt           0          0  ...      4     0        0        0       1   \n",
       "vir              0          0  ...      2     1        0        0       0   \n",
       "\n",
       "          zip  zipper  zones  zoom  álvarez  \n",
       "bill        0       0      0     2        0  \n",
       "brian       0       1      0     0        0  \n",
       "dave        0       0      0     0        0  \n",
       "fortune     0       0      0     1        0  \n",
       "gabriel     0       0      1     3        3  \n",
       "jim         0       0      0     0        0  \n",
       "kathleen    1       0      0     0        0  \n",
       "oswalt      0       0      0     2        0  \n",
       "vir         0       0      0     0        0  \n",
       "\n",
       "[9 rows x 4857 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.013*\"audience\" + 0.011*\"shit\" + 0.009*\"fuckin\" + 0.007*\"fuck\" + 0.004*\"women\" + 0.003*\"black\" + 0.003*\"horse\" + 0.003*\"men\" + 0.003*\"jesus\" + 0.003*\"ta\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"dogs\" + 0.004*\"fact\" + 0.004*\"jax\" + 0.003*\"mom\" + 0.003*\"dog\" + 0.003*\"martin\" + 0.003*\"mexican\" + 0.003*\"baby\" + 0.003*\"gabriel\" + 0.003*\"canelo\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "for idx, topic in ldana.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"shit\" + 0.004*\"fuck\" + 0.004*\"indian\" + 0.003*\"pubes\" + 0.003*\"beautiful\" + 0.003*\"beef\" + 0.003*\"mom\" + 0.003*\"somebody\" + 0.003*\"jesus\" + 0.003*\"men\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.019*\"fuckin\" + 0.016*\"shit\" + 0.011*\"fuck\" + 0.007*\"women\" + 0.006*\"black\" + 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"men\" + 0.004*\"ta\" + 0.003*\"woman\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.023*\"audience\" + 0.006*\"horse\" + 0.004*\"dog\" + 0.004*\"dogs\" + 0.004*\"laughs\" + 0.004*\"horses\" + 0.004*\"fact\" + 0.003*\"jax\" + 0.003*\"mom\" + 0.003*\"bear\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "for idx, topic in ldana.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.019*\"fuckin\" + 0.015*\"shit\" + 0.008*\"fuck\" + 0.007*\"women\" + 0.004*\"men\" + 0.004*\"woman\" + 0.004*\"beautiful\" + 0.004*\"jax\" + 0.004*\"indian\" + 0.004*\"somebody\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.008*\"shit\" + 0.005*\"black\" + 0.004*\"fuck\" + 0.004*\"ahah\" + 0.004*\"mom\" + 0.004*\"son\" + 0.003*\"martin\" + 0.003*\"fact\" + 0.003*\"dogs\" + 0.003*\"long\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.050*\"audience\" + 0.013*\"horse\" + 0.008*\"horses\" + 0.008*\"laughs\" + 0.006*\"cheese\" + 0.006*\"bear\" + 0.006*\"marathon\" + 0.005*\"dog\" + 0.005*\"appendix\" + 0.004*\"summer\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.006*\"pubes\" + 0.004*\"hell\" + 0.004*\"fuck\" + 0.004*\"barn\" + 0.004*\"books\" + 0.004*\"weird\" + 0.003*\"sex\" + 0.003*\"store\" + 0.003*\"music\" + 0.003*\"shutdown\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "for idx, topic in ldana.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.050*\"audience\" + 0.013*\"horse\" + 0.009*\"laughs\" + 0.009*\"horses\" + 0.006*\"cheese\" + 0.006*\"bear\" + 0.006*\"marathon\" + 0.005*\"dog\" + 0.005*\"appendix\" + 0.004*\"summer\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.013*\"shit\" + 0.010*\"fuckin\" + 0.008*\"fuck\" + 0.005*\"women\" + 0.004*\"black\" + 0.004*\"woman\" + 0.003*\"men\" + 0.003*\"fact\" + 0.003*\"beautiful\" + 0.002*\"gay\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.005*\"mom\" + 0.005*\"missouri\" + 0.005*\"jesus\" + 0.005*\"madigan\" + 0.004*\"information\" + 0.004*\"state\" + 0.004*\"somebody\" + 0.004*\"ha\" + 0.004*\"kathleen\" + 0.004*\"seat\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.005*\"map\" + 0.004*\"hell\" + 0.003*\"tower\" + 0.003*\"book\" + 0.003*\"today\" + 0.003*\"questions\" + 0.003*\"goin\" + 0.003*\"bag\" + 0.003*\"news\" + 0.003*\"brian\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "for idx, topic in ldana.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general a chunksize of 100k and update_every set to 1 is equivalent to a chunksize of 50k and update_every set to 2. The primary difference is that we will save some memory using the smaller chunksize, but we will be doing multiple loading/processing steps prior to moving onto the maximization step. Passes are not related to chunksize or update_every. Passes is the number of times we want to go through the entire corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('audience', 0.04996114),\n",
       "   ('horse', 0.013201391),\n",
       "   ('laughs', 0.008533326),\n",
       "   ('horses', 0.008533193),\n",
       "   ('cheese', 0.0064914036),\n",
       "   ('bear', 0.006491213),\n",
       "   ('marathon', 0.005907495),\n",
       "   ('dog', 0.005032507),\n",
       "   ('appendix', 0.0050324616),\n",
       "   ('summer', 0.004449031)]),\n",
       " (1,\n",
       "  [('shit', 0.01268701),\n",
       "   ('fuckin', 0.009966808),\n",
       "   ('fuck', 0.0077566616),\n",
       "   ('women', 0.004951479),\n",
       "   ('black', 0.003846392),\n",
       "   ('woman', 0.0036763763),\n",
       "   ('men', 0.003336364),\n",
       "   ('fact', 0.0029963362),\n",
       "   ('beautiful', 0.0025713067),\n",
       "   ('gay', 0.0024863072)]),\n",
       " (2,\n",
       "  [('mom', 0.004910204),\n",
       "   ('missouri', 0.004909975),\n",
       "   ('jesus', 0.0045881514),\n",
       "   ('madigan', 0.0045880014),\n",
       "   ('information', 0.003944278),\n",
       "   ('state', 0.0039442615),\n",
       "   ('somebody', 0.0039442102),\n",
       "   ('ha', 0.003944083),\n",
       "   ('kathleen', 0.0039440533),\n",
       "   ('seat', 0.0036222276)]),\n",
       " (3,\n",
       "  [('map', 0.0045540878),\n",
       "   ('hell', 0.0035695217),\n",
       "   ('tower', 0.0030772723),\n",
       "   ('book', 0.0030772043),\n",
       "   ('today', 0.0030771918),\n",
       "   ('questions', 0.0030771785),\n",
       "   ('goin', 0.0030771666),\n",
       "   ('bag', 0.0030771527),\n",
       "   ('news', 0.0030771275),\n",
       "   ('brian', 0.0030770113)])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldana.show_topics(num_words = 10, formatted = False)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: Social Media\n",
    "* Topic 1: Racism\n",
    "* Topic 2: Profanity\n",
    "* Topic 3: Creature,humour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'bill'),\n",
       " (3, 'brian'),\n",
       " (1, 'dave'),\n",
       " (1, 'fortune'),\n",
       " (1, 'gabriel'),\n",
       " (0, 'jim'),\n",
       " (2, 'kathleen'),\n",
       " (1, 'oswalt'),\n",
       " (1, 'vir')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: Social Media [Brian,fortune]\n",
    "* Topic 1: Racism [dave,gabriel]\n",
    "* Topic 2: Profanity[Bill,Kathleen]\n",
    "* Topic 3: Creature,humour[Jim, Oswalt,Vir]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.030*\"audience\" + 0.008*\"horse\" + 0.007*\"shit\" + 0.006*\"horses\" + 0.005*\"black\" + 0.005*\"laughs\" + 0.005*\"ahah\" + 0.004*\"fuck\" + 0.004*\"cheese\" + 0.004*\"bear\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.010*\"jax\" + 0.005*\"picture\" + 0.004*\"woman\" + 0.004*\"ice\" + 0.004*\"message\" + 0.004*\"gary\" + 0.004*\"craig\" + 0.003*\"door\" + 0.003*\"mom\" + 0.003*\"large\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"shit\" + 0.006*\"indian\" + 0.005*\"fuck\" + 0.005*\"pubes\" + 0.005*\"beautiful\" + 0.004*\"beef\" + 0.004*\"india\" + 0.004*\"men\" + 0.003*\"sex\" + 0.003*\"barn\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.015*\"fuckin\" + 0.009*\"shit\" + 0.006*\"fuck\" + 0.004*\"women\" + 0.004*\"mom\" + 0.003*\"dad\" + 0.003*\"somebody\" + 0.003*\"baby\" + 0.003*\"fact\" + 0.003*\"dogs\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana1 = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80,chunksize  = 1000,update_every=50)\n",
    "for idx, topic in ldana1.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: Nature \n",
    "* Topic 1: Profanity\n",
    "* Topic 2: Family\n",
    "* Topic 3: Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('audience', 0.029598042),\n",
       "   ('horse', 0.008242832),\n",
       "   ('shit', 0.0068760007),\n",
       "   ('horses', 0.0056801923),\n",
       "   ('black', 0.0053385906),\n",
       "   ('laughs', 0.0049968944),\n",
       "   ('ahah', 0.0046551735),\n",
       "   ('fuck', 0.004313458),\n",
       "   ('cheese', 0.0041426886),\n",
       "   ('bear', 0.0039718687)]),\n",
       " (1,\n",
       "  [('jax', 0.009972306),\n",
       "   ('picture', 0.005233275),\n",
       "   ('woman', 0.004048258),\n",
       "   ('ice', 0.0040482245),\n",
       "   ('message', 0.004048168),\n",
       "   ('gary', 0.0040480904),\n",
       "   ('craig', 0.0036531426),\n",
       "   ('door', 0.0028633873),\n",
       "   ('mom', 0.0028633543),\n",
       "   ('large', 0.0028633084)]),\n",
       " (2,\n",
       "  [('shit', 0.009434548),\n",
       "   ('indian', 0.0055463975),\n",
       "   ('fuck', 0.005317547),\n",
       "   ('pubes', 0.0050887973),\n",
       "   ('beautiful', 0.0046314895),\n",
       "   ('beef', 0.0041740933),\n",
       "   ('india', 0.00394522),\n",
       "   ('men', 0.0037166039),\n",
       "   ('sex', 0.003259156),\n",
       "   ('barn', 0.0030303919)]),\n",
       " (3,\n",
       "  [('fuckin', 0.015483405),\n",
       "   ('shit', 0.009144757),\n",
       "   ('fuck', 0.0063715777),\n",
       "   ('women', 0.0042587123),\n",
       "   ('mom', 0.0037304915),\n",
       "   ('dad', 0.0033343215),\n",
       "   ('somebody', 0.0032022658),\n",
       "   ('baby', 0.003070235),\n",
       "   ('fact', 0.0030701954),\n",
       "   ('dogs', 0.0030701912)])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldana1.show_topics(num_words = 10, formatted = False)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"shit\" + 0.008*\"fuck\" + 0.006*\"black\" + 0.005*\"men\" + 0.005*\"ahah\" + 0.004*\"women\" + 0.004*\"indian\" + 0.004*\"woman\" + 0.004*\"fucking\" + 0.004*\"pubes\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.005*\"jax\" + 0.005*\"dogs\" + 0.005*\"fact\" + 0.004*\"martin\" + 0.004*\"mexican\" + 0.004*\"mom\" + 0.004*\"gabriel\" + 0.004*\"canelo\" + 0.003*\"frankie\" + 0.003*\"voice\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.005*\"missouri\" + 0.005*\"mom\" + 0.005*\"madigan\" + 0.005*\"jesus\" + 0.004*\"kathleen\" + 0.004*\"information\" + 0.004*\"somebody\" + 0.004*\"state\" + 0.004*\"ha\" + 0.004*\"seat\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.027*\"audience\" + 0.019*\"fuckin\" + 0.009*\"shit\" + 0.008*\"horse\" + 0.007*\"fuck\" + 0.005*\"horses\" + 0.005*\"ta\" + 0.005*\"dog\" + 0.005*\"laughs\" + 0.005*\"women\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana1 = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80,chunksize  = 1000,update_every=50,alpha = 'auto')\n",
    "for idx, topic in ldana1.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def verbs(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only Verbs.'''\n",
    "    is_verb = lambda pos: pos[:2] == 'VB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    is_verb = [word for (word, pos) in pos_tag(tokenized) if is_verb(pos)] \n",
    "    return ' '.join(is_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>ladies and gentlemen bill burr  all right th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>♪    lets give a big warm welcome to mr brian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>please welcome fortune feimster ♪ im a powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>can you please state your name martin moreno ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>audience cheering applauding thank you thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>whoo   ♪ kathleen ♪ ♪ madigan ♪ ♪ kathleen ♪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>hello denver   oh my god hello thank you tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>i lost  of my mind its very freeing you should...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "bill        ladies and gentlemen bill burr  all right th...\n",
       "brian     ♪    lets give a big warm welcome to mr brian ...\n",
       "dave      this is dave he tells dirty jokes for a living...\n",
       "fortune      please welcome fortune feimster ♪ im a powe...\n",
       "gabriel    can you please state your name martin moreno ...\n",
       "jim       audience cheering applauding thank you thank y...\n",
       "kathleen    whoo   ♪ kathleen ♪ ♪ madigan ♪ ♪ kathleen ♪...\n",
       "oswalt      hello denver   oh my god hello thank you tha...\n",
       "vir       i lost  of my mind its very freeing you should..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>burr thank thank thank are goin guys standin s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>give mr thank thank is coming appreciate being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>is dave tells is happens signifies fire fear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>please im get want get thats want im get want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>please know ive been touring been screaming hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>cheering applauding thank oh thank coming chee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>kathleen shes coming ♪ gon be put think have s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>hello thank thank thank oh coming broke thats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>lost freeing see faces are excited is were gon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "bill      burr thank thank thank are goin guys standin s...\n",
       "brian     give mr thank thank is coming appreciate being...\n",
       "dave      is dave tells is happens signifies fire fear t...\n",
       "fortune   please im get want get thats want im get want ...\n",
       "gabriel   please know ive been touring been screaming hu...\n",
       "jim       cheering applauding thank oh thank coming chee...\n",
       "kathleen  kathleen shes coming ♪ gon be put think have s...\n",
       "oswalt    hello thank thank thank oh coming broke thats ...\n",
       "vir       lost freeing see faces are excited is were gon..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_verb = pd.DataFrame(data_clean.transcript.apply(verbs))\n",
    "data_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaah</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>absorb</th>\n",
       "      <th>absorbs</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>accomplished</th>\n",
       "      <th>according</th>\n",
       "      <th>accounted</th>\n",
       "      <th>...</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youyou</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>ándale</th>\n",
       "      <th>ñañaras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oswalt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vir</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 2053 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaah  abducted  abhorring  absorb  absorbs  accept  accepted  \\\n",
       "bill         0         0          0       1        0       0         0   \n",
       "brian        0         0          0       0        0       0         0   \n",
       "dave         1         0          0       0        0       0         0   \n",
       "fortune      0         0          0       0        0       0         0   \n",
       "gabriel      0         0          0       0        0       1         1   \n",
       "jim          0         0          0       0        0       0         0   \n",
       "kathleen     0         1          1       0        0       2         0   \n",
       "oswalt       0         0          0       0        0       0         0   \n",
       "vir          0         1          0       0        1       2         0   \n",
       "\n",
       "          accomplished  according  accounted  ...  youd  youll  youtube  \\\n",
       "bill                 0          0          0  ...     1      0        0   \n",
       "brian                0          0          0  ...     0      0        0   \n",
       "dave                 0          0          0  ...     2      1        0   \n",
       "fortune              1          0          0  ...     0      0        0   \n",
       "gabriel              0          1          0  ...     0      0        0   \n",
       "jim                  1          0          0  ...     0      1        2   \n",
       "kathleen             0          0          0  ...     0      0        0   \n",
       "oswalt               0          0          1  ...     0      0        0   \n",
       "vir                  2          0          0  ...     0      1        0   \n",
       "\n",
       "          youyou  zero  zoned  zoom  zoomed  ándale  ñañaras  \n",
       "bill           0     0      1     0       0       0        0  \n",
       "brian          1     0      0     0       0       0        0  \n",
       "dave           0     0      0     0       0       0        0  \n",
       "fortune        0     1      0     0       1       0        0  \n",
       "gabriel        0     0      0     1       0       1        1  \n",
       "jim            0     0      0     0       0       0        0  \n",
       "kathleen       0     0      0     0       0       0        0  \n",
       "oswalt         0     0      0     0       0       0        0  \n",
       "vir            0     0      0     0       0       0        0  \n",
       "\n",
       "[9 rows x 2053 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.5)\n",
    "data_cvna = cvna.fit_transform(data_verb.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_verb.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80,chunksize  = 1000,update_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"fuckin\" + 0.007*\"fucked\" + 0.005*\"feeling\" + 0.004*\"huh\" + 0.004*\"lie\" + 0.003*\"answer\" + 0.003*\"wake\" + 0.003*\"goin\" + 0.003*\"write\" + 0.003*\"laying\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.010*\"broke\" + 0.010*\"dave\" + 0.009*\"ahah\" + 0.008*\"suffer\" + 0.006*\"saves\" + 0.006*\"fucked\" + 0.005*\"scared\" + 0.005*\"rapes\" + 0.005*\"chappelle\" + 0.005*\"raped\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.005*\"explain\" + 0.005*\"grab\" + 0.005*\"reached\" + 0.005*\"kevin\" + 0.005*\"grabs\" + 0.005*\"iced\" + 0.004*\"apologize\" + 0.004*\"married\" + 0.004*\"trickortreating\" + 0.003*\"open\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.069*\"laughs\" + 0.015*\"applauding\" + 0.015*\"cheering\" + 0.006*\"lie\" + 0.005*\"kathleen\" + 0.004*\"removed\" + 0.004*\"treat\" + 0.004*\"spend\" + 0.004*\"wearing\" + 0.004*\"given\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "\n",
    "for idx, topic in ldana2.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('fuckin', 0.013616361),\n",
       "   ('fucked', 0.0074966303),\n",
       "   ('feeling', 0.0050488915),\n",
       "   ('huh', 0.0044368277),\n",
       "   ('lie', 0.0038246536),\n",
       "   ('answer', 0.003213005),\n",
       "   ('wake', 0.0032128824),\n",
       "   ('goin', 0.0032128552),\n",
       "   ('write', 0.0032127816),\n",
       "   ('laying', 0.0026009628)]),\n",
       " (1,\n",
       "  [('broke', 0.0095995115),\n",
       "   ('dave', 0.009599039),\n",
       "   ('ahah', 0.008815434),\n",
       "   ('suffer', 0.008032081),\n",
       "   ('saves', 0.0056810123),\n",
       "   ('fucked', 0.0056809653),\n",
       "   ('scared', 0.0048978655),\n",
       "   ('rapes', 0.004897406),\n",
       "   ('chappelle', 0.004897406),\n",
       "   ('raped', 0.004897406)]),\n",
       " (2,\n",
       "  [('explain', 0.005329339),\n",
       "   ('grab', 0.0046832524),\n",
       "   ('reached', 0.004683246),\n",
       "   ('kevin', 0.0046830564),\n",
       "   ('grabs', 0.004683034),\n",
       "   ('iced', 0.0046830326),\n",
       "   ('apologize', 0.0040374002),\n",
       "   ('married', 0.0040371604),\n",
       "   ('trickortreating', 0.0040370803),\n",
       "   ('open', 0.0033914)]),\n",
       " (3,\n",
       "  [('laughs', 0.06926223),\n",
       "   ('applauding', 0.015443042),\n",
       "   ('cheering', 0.014831289),\n",
       "   ('lie', 0.0056572985),\n",
       "   ('kathleen', 0.005045477),\n",
       "   ('removed', 0.004434111),\n",
       "   ('treat', 0.003822585),\n",
       "   ('spend', 0.0038225632),\n",
       "   ('wearing', 0.0038224743),\n",
       "   ('given', 0.0038224738)])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldana2.show_topics(num_words = 10, formatted = False)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: Behaivour\n",
    "* Topic 1: Interactions.\n",
    "* Topic 2: Adversity.\n",
    "* Topic 3: Entertainment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'bill'),\n",
       " (0, 'brian'),\n",
       " (1, 'dave'),\n",
       " (2, 'fortune'),\n",
       " (2, 'gabriel'),\n",
       " (3, 'jim'),\n",
       " (3, 'kathleen'),\n",
       " (1, 'oswalt'),\n",
       " (0, 'vir')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana2[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: Behaivour[Brain,Vir]\n",
    "* Topic 1: Interactions[fortune,Gabriel,Kathleen]\n",
    "* Topic 2: Adversity.[Dave,Oswalt]\n",
    "* Topic 3: Entertainment.[Bill,Jim]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
